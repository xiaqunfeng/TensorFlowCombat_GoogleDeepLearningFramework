{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.设置输入和输出节点的个数,配置神经网络的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NODE = 784     # 输入节点。图片像素28*28\n",
    "OUTPUT_NODE = 10     # 输出节点。类别数目\n",
    "LAYER1_NODE = 500    # 隐藏层节点数       \n",
    "                              \n",
    "BATCH_SIZE = 100     # 每次batch打包的样本个数        \n",
    "\n",
    "# 模型相关的参数\n",
    "LEARNING_RATE_BASE = 0.8     # 基础学习率 \n",
    "LEARNING_RATE_DECAY = 0.99   # 学习率的衰减率\n",
    "REGULARAZTION_RATE = 0.0001  # 描述模型复杂度的正则化项在损失函数中的系数\n",
    "TRAINING_STEPS = 5000        # 训练轮数\n",
    "MOVING_AVERAGE_DECAY = 0.99  # 滑动平均衰减率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 定义辅助函数来计算前向传播结果，使用ReLU做为激活函数。\n",
    "给定神经网络的输入和所有参赛，计算神经网络的前向传播结果。通过ReLU激活函数实现去线性化。\n",
    "\n",
    "因为在计算损失函数时会一并计算softmax函数，所以，在计算输出层的前向传播结果时，不需要加入激活函数。而且不会影响预测结果，因为预测时使用的是不同类别对应节点输出值的相对大小，有没有softmax层对最后的分类结果的计算没有影响。于是，在计算整个神经网络的前向传播时可以不加入最后的softmax层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 不使用滑动平均类\n",
    "    if avg_class == None:\n",
    "        # 计算隐藏层的前向传播结果\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        # 计算输出层的前向传播结果\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "\n",
    "    else:\n",
    "        # 使用avg_class.average计算出变量的滑动平均值，然后再计算相应的神经网络前向传播结果\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 定义训练过程。\n",
    "**计算交叉熵**\n",
    "\n",
    "`tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_))`\n",
    "\n",
    "* y：神经网络不包含softmax层的前向传播结果\n",
    "* y_：训练数据的正确答案\n",
    "* tf.argmax(y_)：在这里，因为正确答案是一个长度为10的一维数组，而该函数需要提供的是一个正确答案的数字，所以需要tf.argmax函数来得到正确答案对应的类别编号\n",
    "\n",
    "函数是将softmax和cross_entropy放在一起计算，对于分类问题而言，最后一般都是一个单层全连接神经网络，比如softmax分类器居多。\n",
    "\n",
    "对该函数而言，tensorflow神经网络中是没有softmax层，而是在这个函数中进行softmax函数的计算。该函数是直接使用标签数据，而不是采用one-hot编码形式。\n",
    "\n",
    "当分类问题只有一个正确答案时，可以使用这个函数来加速交叉熵的计算。\n",
    "\n",
    "**tf.argmax(average_y, 1)**\n",
    "\n",
    "计算每一个样例的预测答案。average_y是一个batch_size*10的二维数组，每一行表示一个样例的前向传播结果。tf.argmax的第二个参数“1”表示选取最大值的操作仅在第一个维度中进行，即只在每一行选取最大值对应的下标。\n",
    "\n",
    "所以该运算得到的结果是一个长度为batch的一维数组，值表示每个样例的数字识别结果。\n",
    "\n",
    "tf.equal判断两个张量的每一维是否相等，相等返回true，否则返回false。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    # 生成隐藏层的参数。\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    # 生成输出层的参数。\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "    # 计算不含滑动平均类的前向传播结果\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 定义存储训练轮数的变量。因为不需要计算滑动平均值，所以指定为不可训练的变量（trainable=False）\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    # 在所有打标神经网络参数的变量上使用滑动平均\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    # 计算使用滑动平均之后的前向传播结果\n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 计算交叉熵及其平均值\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    # 计算在当前batch中所有样例的交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    # 计算模型的正则化损失。一般只计算神经网络边上权重的正则化损失，而不使用偏置项\n",
    "    regularaztion = regularizer(weights1) + regularizer(weights2)\n",
    "    # 总损失等于交叉熵损失和正则化损失之和\n",
    "    loss = cross_entropy_mean + regularaztion\n",
    "    \n",
    "    # 设置指数衰减的学习率。\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, # 过完所有训练数据需要迭代的次数\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    \n",
    "    # 优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # 在训练神经网络模型时，每过一遍数据即需要通过反向传播来更新神经网络中的参数，\n",
    "    # 又需要更新每一个参数的滑动平均值。\n",
    "    # tf中提供了两种方法来一次完成多个操作：tf.control_dependencies 和 tf.group\n",
    "    # with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "    #     train_op = tf.no_op(name='train')\n",
    "    train_op = tf.group(train_step, variables_averages_op)    \n",
    "\n",
    "    #检验使用滑动平均模型的神经网络前向传播是否正确\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    # 将布尔型的数值转换为实数型，然后计算平均值。该平均值即为模型在一组数据上的正确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 初始化会话，并开始训练过程。\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # 准备验证数据\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        # 准备测试数据\n",
    "        # 注意：在真实应用中，这部分数据在训练时是不可见的，该数据只是作为模型优劣的最后评价标准\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels} \n",
    "        \n",
    "        # 循环的训练神经网络。\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证集上的测试结果\n",
    "            if i % 1000 == 0:\n",
    "                # 计算滑动平均模型在验证数据上的结果\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g \" % (i, validate_acc))\n",
    "            \n",
    "            # 产生这一轮使用的一个batch的训练数据，并运行训练过程\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x:xs,y_:ys})\n",
    "        \n",
    "        # 训练结束后，在测试数据上检测神经网络模型的最终正确率\n",
    "        test_acc=sess.run(accuracy,feed_dict=test_feed)\n",
    "        print((\"After %d training step(s), test accuracy using average model is %g\" %(TRAINING_STEPS, test_acc)))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 主程序入口，这里设定模型训练次数为5000次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step(s), validation accuracy using average model is 0.133 \n",
      "After 1000 training step(s), validation accuracy using average model is 0.9784 \n",
      "After 2000 training step(s), validation accuracy using average model is 0.9812 \n",
      "After 3000 training step(s), validation accuracy using average model is 0.9822 \n",
      "After 4000 training step(s), validation accuracy using average model is 0.9846 \n",
      "After 5000 training step(s), test accuracy using average model is 0.9832\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.ERROR)\n",
    "    mnist = input_data.read_data_sets(\"../../datasets/MNIST_data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
