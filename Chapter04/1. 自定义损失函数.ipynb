{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 自定义损失函数\n",
    "y_表示正确结果，y代表预测结果。\n",
    "\n",
    "tf.greater 和 tf.where 方法：\n",
    "\n",
    "tf.greater: 输入是两个张量，函数会比较两个输入张量中每一个元素的大小，并返回比较结果。当输入张量维度不一致时，tf会进行类似numpy的广播操作（broadcasting）处理。\n",
    "\n",
    "tf.where: 有三个参数。第一个为选择条件，为true时，选择第二个参数中的值，否则使用第三个参数中的值。注：判断和选择都是元素级别。\n",
    "\n",
    "**注：书中是 tf.select, 这里需要替换成 tf.where**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True]\n",
      "[4. 3. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "v1 = tf.constant([1.0, 2.0, 3.0, 4.0])\n",
    "v2 = tf.constant([4.0, 3.0, 2.0, 1.0])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "print(tf.greater(v1,v2).eval())\n",
    "print(tf.where(tf.greater(v1, v2), v1, v2).eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例：损失函数对模型训练结果的影响\n",
    "\n",
    "神经网络结构：两个输入，一个输出，中间没有隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 定义神经网络的相关参数和变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "dataset_size = 128\n",
    "\n",
    "# 两个输入节点\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name=\"x-input\")\n",
    "# 回归问题一般只有一个输出节点\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y-input')\n",
    "\n",
    "# 定义了一个单层的神经网络前向传播过程，这里就是简单的加权和\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1), trainable=True)\n",
    "y = tf.nn.tanh(tf.matmul(x, w1))\n",
    "# y = tf.matmul(x, w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 设置自定义的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。\n",
    "loss_less = 10\n",
    "loss_more = 1\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), \n",
    "                              (y - y_) * loss_more, \n",
    "                              (y_ - y) * loss_less))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 生成模拟数据集。\n",
    "设置回归的正确值为两个输入的和加上一个随机量。\n",
    "\n",
    "之所以加上一个随机量是为了加入不可预测的噪音，否则不同损失函数的意义就不大了，因为不同损失函数都会在能完全预测正确的时候最低。\n",
    "\n",
    "一般来说噪音为一个均值为0的小量，所以这里的噪声设置为 -0.05~0.05 的随机数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm = RandomState(1)\n",
    "X = rdm.rand(dataset_size,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), w1 is: \n",
      "[[-0.81031823]\n",
      " [ 1.4855988 ]] \n",
      "\n",
      "After 1000 training step(s), w1 is: \n",
      "[[-0.02263279]\n",
      " [ 2.1988971 ]] \n",
      "\n",
      "After 2000 training step(s), w1 is: \n",
      "[[0.5252797]\n",
      " [2.5851707]] \n",
      "\n",
      "After 3000 training step(s), w1 is: \n",
      "[[0.9152925]\n",
      " [2.815458 ]] \n",
      "\n",
      "After 4000 training step(s), w1 is: \n",
      "[[1.1496375]\n",
      " [2.961983 ]] \n",
      "\n",
      "Final w1 is: \n",
      " [[1.3268394]\n",
      " [3.0807076]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*batch_size) % dataset_size\n",
    "        end = (i*batch_size) % dataset_size + batch_size\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training step(s), w1 is: \" % (i))\n",
    "            print(sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 重新定义损失函数，使得预测多了的损失大，于是模型应该偏向少的方向预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), w1 is: \n",
      "[[-0.81031823]\n",
      " [ 1.4835987 ]] \n",
      "\n",
      "After 1000 training step(s), w1 is: \n",
      "[[0.05703859]\n",
      " [1.3401766 ]] \n",
      "\n",
      "After 2000 training step(s), w1 is: \n",
      "[[0.7567741]\n",
      " [1.1615651]] \n",
      "\n",
      "After 3000 training step(s), w1 is: \n",
      "[[1.0743011]\n",
      " [1.0788801]] \n",
      "\n",
      "After 4000 training step(s), w1 is: \n",
      "[[1.073686]\n",
      " [1.078645]] \n",
      "\n",
      "Final w1 is: \n",
      " [[1.0749779]\n",
      " [1.0778998]]\n"
     ]
    }
   ],
   "source": [
    "loss_less = 1\n",
    "loss_more = 10\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), \n",
    "                              (y - y_) * loss_more, \n",
    "                              (y_ - y) * loss_less))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*batch_size) % dataset_size\n",
    "        end = (i*batch_size) % dataset_size + batch_size\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training step(s), w1 is: \" % (i))\n",
    "            print(sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 定义损失函数为MSE。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), w1 is: \n",
      "[[-0.81031823]\n",
      " [ 1.4855988 ]] \n",
      "\n",
      "After 1000 training step(s), w1 is: \n",
      "[[-0.14825174]\n",
      " [ 2.0850313 ]] \n",
      "\n",
      "After 2000 training step(s), w1 is: \n",
      "[[0.24612428]\n",
      " [2.3461459 ]] \n",
      "\n",
      "After 3000 training step(s), w1 is: \n",
      "[[0.5294681]\n",
      " [2.4651117]] \n",
      "\n",
      "After 4000 training step(s), w1 is: \n",
      "[[0.7528874]\n",
      " [2.4787993]] \n",
      "\n",
      "Final w1 is: \n",
      " [[0.9403336]\n",
      " [2.384106 ]]\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.mean_squared_error(y, y_)\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*batch_size) % dataset_size\n",
    "        end = (i*batch_size) % dataset_size + batch_size\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training step(s), w1 is: \" % (i))\n",
    "            print(sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
